[{"content":" \u0026ldquo;The only truth is music.\u0026rdquo;\n— Jack Kerouac\n Relative Global Attention - Music Transformer \r","permalink":"https://nd15.github.io/notes/posts/musictransformer/","summary":"Understanding Relative Global Attention","title":"Relative Global Attention - Music Transformer"},{"content":" \u0026ldquo;Whatever limits us, we call Fate.\u0026rdquo;\n— Ralph Waldo Emerson\n Adversarial Audio Synthesis With the rise of GANs and the possibilities they offer, the question of whether or not they are capable of being successfully applied to the field of audio synthesis has arisen. GANs have brought about a revolution in the field of computer vision for a variety of applications, including the generation of images and the enhancement of picture resolution (image super resolution). Research in the fascinating topic of audio synthesis, which has numerous useful applications in the field of natural language processing, is an area that has attracted a lot of attention recently. The operation of GANs on image-like representations of audio, such as spectrograms, would be a simplistic approach to the problem of applying GANs to audio. Nevertheless, because spectrograms are not capable of being inverted, using this method results in lossy estimations, which makes it impossible to listen to the data.\nAutoregressive models, as opposed to GANs, such as WaveNet and SampleRNN operate on raw audio to produce high quality output; nevertheless, this process is however slow because the output audio samples have to be fed back into the model. GANs are an alternative to autoregressive models. In the paper [1] written by Christian Donahue, the authors present two distinct methods/strategies for using GANs to generate one-second chunks of audio. These strategies are as follows: - SpecGAN - WaveGAN Both these approaches use the DCGAN architecture for the purpose of audio synthesis. Their experiments showed that both WaveGAN and SpecGAN can generate spoken digits that are intelligible to humans.\nSpecGAN, A Frequency Domain Audio Generation Model SpecGAN is the frequency domain audio generation model. It uses spectrograms as audio representations for image generation and can be approximately inverted. The spectrograms are of size of 128x128 which yields 16384 samples.In order to process the audio into suitable spectrogram, the audio preprocessing pipeline is given as:\n STFT of the audio with a window size of 16ms and a hop length of 8 ms and mel channels of 128. Magnitude of the resultant spectra Scaling amplitude values logarithmically to better align with human perception Normalizing each frequency bin to have zero mean and unit variance  Evaluation and Results The authors use the inception score and nearest neighbor comparisons for quantitative evaluation and human judgment for qualitative evaluation when assessing the quality of the models. The phase shuffle, which was presented in this paper, was thought to be a valuable addition to the WaveGAN model, whereas in SpecGAN it did not achieve very significant results. SpecGAN had a better performance than WaveGAN when it came to quantitative measures, but WaveGAN was the better choice when it came to qualitative measures.\nMy Results Upto 400 epochs\n   0 100 200 300 400                                         Implementation in Tensorflow 2 Link : https://github.com/ND15/SpecGAN\nReferences  https://arxiv.org/abs/1802.04208 https://github.com/chrisdonahue/wavegan https://github.com/naotokui/SpecGAN https://medium.com/octavian-ai/a-simple-explanation-of-the-inception-score-372dff6a8c7a  NOTE: The training is not yet completed and the results will be updated soon. This is a work in progress.\n","permalink":"https://nd15.github.io/notes/posts/specgan/","summary":"My Notes on the SpecGAN, a audio generation model","title":"SpecGAN - GAN for audio generation"},{"content":"Architecture SegNet is deep convolutional neural network which consists of corresponding encoder and decoder for pixel-wise semantic segmentation. The encoder encodes or downsamples the input images into low resolution feature maps and the decoder network upsamples these low resolution encoder feature maps into the original resolution feature maps. It is similar to other architectures such as UNet and DeconvNet, however it differs from them in some ways. Segnet was designed to be an efficient architecture for pixel-wise segmentation and was motivated for the use in road scene understanding.\nEncoder - Decoder Network \u0026nbsp;\u0026nbsp;The encoder architecture is identical to the convolutional layers in VGG16 architecture. It discards the fully dense connected layers and keeps the 13 convolutional layers. Each encoder block consists of convolutional layers which produces a set of features maps, a batch normalization layer and a ReLU activation layer. It is then followed by a max pooling layer. The decoder also has the same number of convolutional layers. Each decoder architecture is similar to that of the encoder and only varies in terms of the upsampling layer. In SegNet, downsampling is achieved by using max pooling layers after each encoder block. It has a pooling window size of 2 with a stride of 2. The model explores the idea of storing the max-pooling indices in each pooling window for each feature map. The upsampling layer in each corresponding decoder block upsamples the features maps by using the max-pooling indices. Output The output of the final decoder of SegNet is fed into a trainable softmax classifier to predict the probabilities for the K number of classes. The output is a K channel image where each channel represents a class.\nHow\u0026rsquo;s it different As stated earlier, SegNet has similarity with DeconvNet and U-Net in terms architecture. As compared to SegNet, DeconvNet requires more computational resources and is harder to train and in terms of U-Net, it does not reuse pooling indices but instead transfers the entire feature map to the corresponding decoders and concatenates them to upsampled decoder feature maps.\nImplementation I have implemented the SegNet architecture in Tensorflow 2 and it can be found here\nRun the notebook \nOutput Images    Input Predicted          ","permalink":"https://nd15.github.io/notes/posts/segnet/","summary":"My Notes on the SegNet semantic segmentation architecture","title":"Notes on SegNet - A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation"},{"content":"Recurrent Neural Network(RNN) RNN Diagram Internal Architecture of RNN Internal RNN Architecture\n    Forward Propagation  In forward propagation, the input data is fed into the RNN in a forward direction to calculate the output at each timestep, for simplicity assume that each timestep is basically a word in a sentence so first timestep would indicate the input of the first word to the network, the input data passed through the input and hidden layer to the output layer where the output is predicted. For that predicted output the loss is calculated with the help of a loss function. Here we assume a few things, first we assume that the activation function in the hidden layer is the tanh(tangent hyperbolic) function and the output activation function is the softmax function. The softmax function gives us the normalized probabilities from the output. The equations for softmax and tanh functions are given below:\n Softmax function $$ \\textrm{softmax}({z_i}) = \\frac{e^{z_i}}{\\sum_{k=1}^{K}e^{z_k}} $$ Tanh function $$ \\tanh({x}) = \\frac{2}{1+e^{-2x}} - 1 $$  The equations for the forward propagation at any time step t are: $$ {h_t} = \\tanh(U{x_t} + W{h_{t-1}}) $$ $$ {y_t} = \\textrm{softmax}(V{h_t}) $$ $$ {L_t} = -{y_t}\\log\\hat{{y_t}} $$ The argument of the softmax function can also be written as $$ {z_t} = {V{h_t}} $$ Then the equation becomes $$ {y_t} = \\textrm{softmax}(z_t) $$\n The total loss for any given input sequence \\(\\textrm{x}\\) is the summation of all the losses over every time step given by \\(\\sum_{t=0}^{T}{L_t}\\). Backpropagation Through Time(BPTT) Back-propagation involves propagating the error from the output layer to the input layer. Backpropagation helps reduce the error by adjusting the weights between the layers. In back-propagation we pass the calculated gradients backward through the network in reverse order in order to update the weights of each layer which in fact effects the output of the network. This propagation of the gradients is called backpropagation through time because in RNN we are calculating the gradients w.r.t the current timestep and then passing it to the previous timesteps, this can be evidently seen when deriving the update rules for the weights matrices. In recurrent neural networks the trainable parameters are the weights U, V and W and hence they need to be updated. The updation of this weights can be done by the following set of equations: $$ {W} = {W} - \\alpha{\\frac{\\partial L}{\\partial W}} $$ $$ {V} = {U} - \\alpha{\\frac{\\partial L}{\\partial V}} $$ $$ {U} = {U} - \\alpha{\\frac{\\partial L}{\\partial U}} $$\n Here \\(\\alpha \\) is the learning rate. Now, deriving the update rules for the weights can be in the following way:\nUpdate Rules for V $$ \\begin{align} {\\frac{\\partial {L}}{\\partial V}} \u0026amp; = {\\frac{\\partial {L_0}}{\\partial V}} + {\\frac{\\partial {L_1}}{\\partial V}} + \u0026hellip; + {\\frac{\\partial {L_T}}{\\partial V}} \\\\ \u0026amp; = \\sum_{i=0}^{T}{\\frac{\\partial {L_i}}{\\partial V}} \\\\ \\tag{1.1}\\label{eq:par_L_V} \u0026amp; = \\sum_{i=0}^{T}\\frac{\\partial {L_i}}{\\partial {y_i}}\\frac{\\partial {y_i}}{\\partial {z_i}}\\frac{\\partial {z_i}}{\\partial {V}} \\end{align} $$\nSolving for the first term of \\eqref{eq:par_L_V}\n$$ \\begin{align} \\frac{\\partial {L_i}}{\\partial {y_i}} \u0026amp; = \\frac{\\partial }{\\partial {y_i}}{(-{y_i}\\log \\hat{y_i})} \\\\ \u0026amp; = -{y_i}\\frac{\\partial }{\\partial {y_i}}{\\log \\hat{y_i}}\\\\ \u0026amp; = -{y_i}\\frac{1}{\\hat{y_i}} \\\\ \\tag{1.2}\\label{eq:W} \u0026amp; = -\\frac{y_i}{\\hat{y_i}} \\end{align} $$\n  For solving the second term of \\eqref{eq:par_L_V}, we have to break down the partial derivative into two cases: \\(\\textrm{i} = \\textrm{k}\\) and \\(\\textrm{i} \\neq \\textrm{k}\\). Case 1: $$ \\begin{align} \\frac{\\partial {y_i}}{\\partial {z_k}} \u0026amp; = \\frac{\\partial }{\\partial {z_k}}(\\frac{e^{z_i}}{\\sum_{k=1}^{K}e^{s_k}})\\\\ \u0026amp; = \\frac{e^{z_i}}{\\sum_{k=1}^{K}e^{s_k}} - {e^{z_i}}\\left(\\frac{{e^{z_i}}}{(\\sum_{k=1}^{K}e^{s_k})^2}\\right) \\\\ \u0026amp; = \\frac{e^{z_i}}{\\sum_{k=1}^{K}e^{s_k}} - \\left(\\frac{{e^{z_i}}}{(\\sum_{k=1}^{K}e^{s_k})}\\right)^2 \\\\ \u0026amp; = \\hat{y_i}\\left( 1 - \\hat{y_i}\\right) \\end{align} $$\nCase 2: $$ \\begin{align} \\frac{\\partial {y_i}}{\\partial {z_k}} \u0026amp; = \\frac{\\partial }{\\partial {z_k}}(\\frac{e^{z_i}}{\\sum_{k=1}^{K}e^{s_k}})\\\\ \u0026amp; = \\frac{0 - {e^{z_i}}{e^{z_k}}}{(\\sum_{k=1}^{K}e^{s_k})^2} \\\\ \u0026amp; = -\\hat{y_i}\\hat{y_k} \\end{align} $$\nNow,\n$$ \\begin{align} \\frac{\\partial {L}}{\\partial V} \u0026amp; = \\sum_{i=0}^{T}\\frac{\\partial {L_i}}{\\partial {y_i}}\\frac{\\partial {y_i}}{\\partial {z_i}}\\frac{\\partial {z_i}}{\\partial {V}} \\\\ \\frac{\\partial {L}}{\\partial V} \u0026amp; = \\sum_{i=0}^{T}\\frac{\\partial {L_i}}{\\partial {z_i}}\\frac{\\partial {z_i}}{\\partial {V}} \\\\ \\frac{\\partial {L_i}}{\\partial {z_i}} \u0026amp; = -\\frac{y_i}{\\hat{y_i}}\\begin{cases} \\hat{y_i}\\left( 1 - \\hat{y_i}\\right), \u0026amp; \\text{if $i = k$}.\\\\ -\\hat{y_i}\\hat{y_k}, \u0026amp; \\text{otherwise}. \\end{cases}\\\\ \\tag{1.3}\\label{eq:V_cases} \u0026amp; = \\begin{cases} -{y_i}\\left( 1 - \\hat{y_i}\\right), \u0026amp; \\text{if $i = k$}.\\\\ {y_i}\\hat{y_k}, \u0026amp; \\text{otherwise}. \\end{cases}\\\\ \\end{align} $$\nIn order to get a general expression for eqn \\eqref{eq:V_cases}, we need to sum for all the K number of classes, hence the equation becomes $$ \\begin{align} \\frac{\\partial {L_i}}{\\partial {z_k}} \u0026amp; = -{y_i} + {y_i}\\hat{y_i} + \\sum_{i \\neq k}{y_i}\\hat{y_k} \\\\ \u0026amp; = -{y_k} + {y_k}\\hat{y_k} + \\sum_{i \\neq k}{y_i}\\hat{y_k} \\\\ \u0026amp; = -{y_k} + \\hat{y_k}\\left({y_k} + \\sum_{i \\neq k}{{y_i}}\\right) \\end{align} $$ The part inside the bracket becomes a sum over all the k classes, therefore $$ \\begin{align} \\frac{\\partial {L_i}}{\\partial {z_k}} \u0026amp; = -{y_k} + \\hat{y_k}\\left(\\sum_{i=1}^{k}{{y_i}}\\right) \\\\ \u0026amp; = -{y_k} + \\hat{y_k} \\left(\\because \\sum_{i=1}^{k} = 1\\right) \\end{align} $$\nSolving for the third term of \\eqref{eq:par_L_V} $$ \\begin{align} \\frac{\\partial {z_i}}{\\partial {V}} \u0026amp; = \\frac{\\partial}{\\partial {V}}(V{h_i}) \\\\ \u0026amp; = {h_i} \\end{align} $$\nSubsituiting all these values in eqn \\eqref{eq:par_L_V}, we get\n$$ \\begin{align} \\frac{\\partial {L}}{\\partial {V}} \u0026amp; = \\sum_{i=0}^{T}(\\hat{y_i} - {y_i}) \\otimes {h_i} \\end{align} $$\nUpdate rule for W $$ \\begin{align} {\\frac{\\partial {L}}{\\partial W}} \u0026amp; = {\\frac{\\partial {L_0}}{\\partial W}} + {\\frac{\\partial {L_1}}{\\partial W}} + \u0026hellip; + {\\frac{\\partial {L_T}}{\\partial W}} \\\\ \u0026amp; = \\sum_{i=0}^{T}{\\frac{\\partial {L_i}}{\\partial W}} \\\\ \\tag{2.1}\\label{eq:par_L_W} \u0026amp; = \\sum_{i=0}^{T}\\frac{\\partial {L_i}}{\\partial {y_i}}\\frac{\\partial {y_i}}{\\partial {h_i}}\\frac{\\partial {h_i}}{\\partial {W}} \\end{align} $$\n$$ \\begin{align} {\\frac{\\partial {L_1}}{\\partial W}} \u0026amp; = \\frac{\\partial {L_1}}{\\partial \\hat{y_1}}\\frac{\\partial \\hat{y_1}}{\\partial {h_1}}\\frac{\\partial {h_1}}{\\partial {W}}\\\\ \\tag{2.2}\\label{eq:par_L_2} {\\frac{\\partial {L_2}}{\\partial W}} \u0026amp; = \\frac{\\partial {L_2}}{\\partial \\hat{y_2}}\\frac{\\partial \\hat{y_2}}{\\partial {h_2}}\\frac{\\partial {h_2}}{\\partial {W}}\\\\ \\end{align} $$\n  The term \\({h_2}\\) is defined as \\({h_2}\\) = \\(\\tanh(U{x_2} + W{h_1})\\)  It can be seen that the term \\({h_2}\\) is dependent on the previous timestep's hidden state \\({h_1}\\). So, in order compute the partial derivative of the hidden state w.r.t \\({W}\\) we can split it into two parts - explicit and implicit. The explicit part treats the argument inside the tanh function as a constant while the implicit part moves inside the function and does the derivative of the function. $$ {\\frac{\\partial {h_2}}{\\partial W}} = \\frac{\\partial {h_2^+}}{\\partial {W}} + \\frac{\\partial {h_2}}{\\partial {h_1}}\\frac{\\partial {h_1}}{\\partial {W}} $$ The equation \\eqref{eq:par_L_2} can thus be written as $$ \\begin{align} {\\frac{\\partial {L_2}}{\\partial W}} \u0026amp; = \\frac{\\partial {L_2}}{\\partial \\hat{y_2}}\\frac{\\partial \\hat{y_2}}{\\partial {h_2}} \\left( \\frac{\\partial {h_2^+}}{\\partial {W}} + \\frac{\\partial {h_2}}{\\partial {h_1}}\\frac{\\partial {h_1}}{\\partial {W}}\\right)\\\\ \u0026amp; = \\frac{\\partial {L_2}}{\\partial \\hat{y_2}}\\frac{\\partial \\hat{y_2}}{\\partial {h_2}}\\frac{\\partial {h_2^+}}{\\partial {W}} + \\frac{\\partial {L_2}}{\\partial \\hat{y_2}}\\frac{\\partial \\hat{y_2}}{\\partial {h_2}}\\frac{\\partial {h_2}}{\\partial {h_1}}\\frac{\\partial {h_1}}{\\partial {W}} \\end{align} $$\nGeneralizing this equation it becomes,\n$$ \\begin{align} \\frac{\\partial {L_i}}{\\partial {W}} = \\sum_{k=1}^{i}\\frac{\\partial {L_i}}{\\partial \\hat{y_i}}\\frac{\\partial \\hat{y_i}}{\\partial {h_i}} \\frac{\\partial \\hat{h_i}}{\\partial {h_k}}\\frac{\\partial {h_k^+}}{\\partial {W}} \\\\ \\tag{2.3}\\label{eq:par_L_gen} \\frac{\\partial {L}}{\\partial {W}} = \\sum_{i=1}^{T}\\sum_{k=1}^{i}\\frac{\\partial {L_i}}{\\partial \\hat{y_i}}\\frac{\\partial \\hat{y_i}}{\\partial {h_i}} \\frac{\\partial {h_i}}{\\partial {h_k}}\\frac{\\partial {h_k^+}}{\\partial {W}} \\end{align} $$\n Here, \\(\\frac{\\partial h_i}{\\partial h_k}\\) is a chain rule within itself i.e. if \\(i = 3\\) and \\(k = 1\\), then \\(\\frac{\\partial h_3}{\\partial h_1} = \\frac{\\partial h_3}{\\partial h_2}\\frac{\\partial h_2}{\\partial h_1}\\) Hence the equation \\eqref{eq:par_L_gen} becomes\n$$ \\begin{align} \\frac{\\partial {L}}{\\partial {W}} = \\sum_{i=1}^{T}\\sum_{k=1}^{i}\\frac{\\partial {L_i}}{\\partial \\hat{y_i}}\\frac{\\partial \\hat{y_i}}{\\partial {h_i}}\\left(\\prod_{m=k+1}^{i} \\frac{\\partial {h_m}}{\\partial {h_{m-1}}}\\right)\\frac{\\partial {h_k^+}}{\\partial {W}} \\end{align} $$\n Now at any point m, we know that \\(h_m = \\tanh(Ux_m + Wh_{m-1})\\) and \\(\\frac{\\partial h_m}{\\partial h_{m-1}} = W^Tdiag(1-\\tanh^2(Ux_m + Wh_{m-1}))\\) Also, \\(\\frac{\\partial {L_i}}{\\partial \\hat{y_i}}\\frac{\\partial \\hat{y_i}}{\\partial {h_i}} = \\hat{y_i} - {y_i}\\) and the explicit derivative is \\(\\frac{\\partial {h_k^+}}{\\partial {W}} = {h_{k-1}}\\). There the complete equation is: $$ \\tag{2.4}\\label{eq:complete_W} \\frac{\\partial {L}}{\\partial {W}} = \\sum_{i=1}^{T}\\sum_{k=1}^{i}(\\hat{y_i} - {y_i})\\left(\\prod_{m=k+1}^{i} W^Tdiag(1-\\tanh^2(Ux_m + Wh_{m-1}))\\right)\\otimes {h_{k-1}} $$\nUpdate rule for U  The update rule for \\(U\\) can be derived in the same manner as \\(W\\), the only difference comes in the explicit part of \\(U\\).The explicit derivative is \\(\\frac{\\partial {h_k^+}}{\\partial {U}} = {x_{k}}\\) Hence the complete update rule is: $$ \\tag{2.5}\\label{eq:complete_U} \\frac{\\partial {L}}{\\partial {U}} = \\sum_{i=1}^{T}\\sum_{k=1}^{i}(\\hat{y_i} - {y_i})\\left(\\prod_{m=k+1}^{i} W^Tdiag(1-\\tanh^2(Ux_m + Wh_{m-1}))\\right)\\otimes {x_k} $$\nReferences  Activation Functions in Neural Networks Recurrent Neural Networks Ahlad Kumar\u0026rsquo;s Playlist RNN from scratch\n ","permalink":"https://nd15.github.io/notes/posts/rnn/","summary":"Notes consisting of the complete derivation of the forward and backpropagation rules in RNN.","title":"Notes on Forward and Back-Propagation in RNNs"},{"content":"I am a Machine Learning Engineer with a BTech Degree in CSE. I love to make 3D art in my free time using Blender.\n 💻 Github: Profile  I have also worked in the field of computer vision and have been in this roles previously:\n Machine Learning Engineer(TechVariable) Audio AI Engineer(Meeami Technologies) Computer Vision Intern (Mowito) Research Intern (IIIT-H)    ","permalink":"https://nd15.github.io/notes/about/","summary":"I am a Machine Learning Engineer with a BTech Degree in CSE. I love to make 3D art in my free time using Blender.\n 💻 Github: Profile  I have also worked in the field of computer vision and have been in this roles previously:\n Machine Learning Engineer(TechVariable) Audio AI Engineer(Meeami Technologies) Computer Vision Intern (Mowito) Research Intern (IIIT-H)    ","title":"About"}]